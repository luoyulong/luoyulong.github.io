<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cuda | 零露漙兮]]></title>
  <link href="http://luoyulong.github.io/blog/categories/cuda/atom.xml" rel="self"/>
  <link href="http://luoyulong.github.io/"/>
  <updated>2015-07-12T22:51:59+08:00</updated>
  <id>http://luoyulong.github.io/</id>
  <author>
    <name><![CDATA[luoyulong]]></name>
    <email><![CDATA[luoyulong@ncic.ac.cn]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CUDA 优化经验]]></title>
    <link href="http://luoyulong.github.io/blog/2014/07/03/gpuexperiences/"/>
    <updated>2014-07-03T17:45:10+08:00</updated>
    <id>http://luoyulong.github.io/blog/2014/07/03/gpuexperiences</id>
    <content type="html"><![CDATA[<h1>硬件模型</h1>

<ol>
<li><p>shared-memoy在硬件层是以SM为单位，在逻辑层是以block为单位</p></li>
<li><p>warp是GPU在硬件层的并行单位。一般来说，warp等于32. SM在处理一个block kernel时，会经可能多的发射warp，每个warp内线程的大小为32.</p></li>
<li><p><span id="anchor1"></span>每个SM可以同时驻留多个 block执行（active block），这主要取决于当前SM是否拥有足够的硬件资源，如Register，shared-memory等等.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p></li>
<li><p>在200机器上的GPU，每个SM拥有256*256个32位的寄存器（即平均每个线程有256个32-bite的寄存器），49152byte的shared memory,65536byte的constant memory。</p></li>
</ol>


<h1>优化原则</h1>

<ol>
<li><p>把部分无共享的shared-memory变为global memory， 并没有增加速度，推测是因为对shared-memory减少的还不够 不足以使得同时在SM运行的block增加。（已经验证）</p></li>
<li><p>有些常量经常被用到，且占用的空间很小。之前使用常量存储器，现改用参数传递，放入寄存器进行加速。</p></li>
<li><p>把与threadIdx无关的公共计算提到host端计算，再把结果使用参数传递给kernel函数</p></li>
<li><p>写回的变量不使用shared-memory</p></li>
<li><p>shared-memory和分块大小之间有一个<a href="#anchor1">tradeoff</a>，即越多的shared-memory会导致在同一SM上驻留的block减少，但同时其访存的性能会上升</p></li>
<li><p>除法非常耗时，如果精度允许的话，使用被除数的倒数组成乘法替换除法。</p></li>
<li><p>GPU适合小而多的运算，对于复杂运算（例如许多除法，大尺寸工作集），CPU反而占据性能优势</p></li>
<li>分块的大小：block中总的线程数要大于每个sm中硬件线程的个数，这样才能保证硬件线程都处在工作中。</li>
</ol>


<h1>CUDA手册:性能优化</h1>

<p>Performance optimization revolves around three basic strategies:<em> Maximize parallel execution to achieve maximum utilization;</em> Optimize memory usage to achieve maximum memory throughput;* Optimize instruction usage to achieve maximum instruction throughput</p>

<h2>Maximize Utilization</h2>

<p>XXXX###Application Level
XXXX###Device Level
1. For devices of compute capability 1.x, only one kernel can execute on a device at one time, so the kernel should be launched with at least <strong>as many thread blocks as</strong> there are multiprocessors in the device.2. For devices of compute capability 2.x and higher, multiple kernels can <strong>execute concurrently</strong> on a device, so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently as described in Asynchronous Concurrent Execution.</p>

<h3>Multiprocessor Level</h3>

<p>XXXX</p>

<h2>Maximize Memory Throughput</h2>

<p>XXXX</p>

<h3>Device Memory Accesses</h3>

<p>XXXX</p>

<h4>Shared Memory<em> xxxx</em> Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory.To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously. Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is n times as high as the bandwidth of a single module.</h4>

<ul>
<li>However, if two addresses of a memory request fall in the same memory bank, there is a <strong>bank conflict</strong> and the access has to be serialized. The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests. If the number of separate memory requests is n, the initial memory request is said to cause n-way bank conflicts.</li>
<li>To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts. This is described in Compute Capability 1.x,Compute Capability 2.x, Compute Capability 3.x, and Compute Capability 5.0 for devices of compute capability 1.x, 2.x, 3.x, and 5.0, respectively.</li>
</ul>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>详见http://stackoverflow.com/questions/12212003/how-concurrent-blocks-can-run-a-single-gpu-streaming-multiprocessor/12213137#12213137<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CUDA 计时的方法]]></title>
    <link href="http://luoyulong.github.io/blog/2014/07/02/cuda-timing/"/>
    <updated>2014-07-02T15:04:09+08:00</updated>
    <id>http://luoyulong.github.io/blog/2014/07/02/cuda-timing</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>测试CUDA应用的时间</p>

<h1>解决方案</h1>

<p>引用自stackoverflow：</p>

<blockquote><p>You could do sth along the lines of :</p></blockquote>

<p>``` cuda</p>

<h1>include&lt;sys/time.h></h1>

<p>struct timeval t1, t2;</p>

<p>gettimeofday(&amp;t1, 0);</p>

<p>kernel_call&lt;&lt;&lt;dimGrid, dimBlock, 0>>>();</p>

<p>gpuErrchk(cudaThreadSynchronize());</p>

<p>gettimeofday(&amp;t2, 0);</p>

<p>double time = (1000000.0*(t2.tv_sec-t1.tv_sec) + t2.tv_usec-t1.tv_usec)/1000000.0;</p>

<p>printf(&ldquo;Time to generate:  %3.1f ms \n&rdquo;, time);
<code>
or:
</code>
float time;
cudaEvent_t start, stop;</p>

<p>gpuErrchk( cudaEventCreate(&amp;start) );
gpuErrchk( cudaEventCreate(&amp;stop) );
gpuErrchk( cudaEventRecord(start, 0) );</p>

<p>kernel_call&lt;&lt;&lt;dimGrid, dimBlock, 0>>>();</p>

<p>gpuErrchk( cudaEventRecord(stop, 0) );
gpuErrchk( cudaEventSynchronize(stop) );
gpuErrchk( cudaEventElapsedTime(&amp;time, start, stop) );</p>

<p>printf(&ldquo;Time to generate:  %3.1f ms \n&rdquo;, time);
```</p>

<p>其中，在cpu端计时，由于GPU的kenernl与cpu是异步执行，所以在获得整个cuda应用的结束时间前，必须要调用cudaThreadSynchronize（现替换成cudaDeviceSynchronize），否则测的时间是最后一个kernel 启动的时间。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ROSE 声明CUDA类型的变量]]></title>
    <link href="http://luoyulong.github.io/blog/2014/06/30/rose-cuda-varialbe/"/>
    <updated>2014-06-30T10:26:12+08:00</updated>
    <id>http://luoyulong.github.io/blog/2014/06/30/rose-cuda-varialbe</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>需要声明CUDA类型的变量，如 constant,device,shared等等</p>

<h1>解决方案</h1>

<p>调用<code>get_declarationModifier().get_storageModifier().set</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[error: no instance of overloaded function "tex1Dfetch" matches the argument list argument types are: (texture<double, 1, cudaReadModeElementType>, int)  ]]></title>
    <link href="http://luoyulong.github.io/blog/2014/06/29/cudatexture/"/>
    <updated>2014-06-29T15:59:52+08:00</updated>
    <id>http://luoyulong.github.io/blog/2014/06/29/cudatexture</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>在使用cuda的纹理存储器时，编译器报错
<code>bash
stencilexample.cu(147): error:
no instance of overloaded function "tex1Dfetch"
matches the argument list argument types are:
(texture&lt;double, 1, cudaReadModeElementType&gt;, int)
</code></p>

<h1>解决方法</h1>

<p>经过研究，发现原来cuda的纹理存储器不能直接支持double</p>

<blockquote><p>The type of a texel, which is restricted to the basic integer and single-precision floating-point types and any of the 1-, 2-, and 4-component vector types defined in char, short, int, long, longlong, float, double that are derived from the basic integer and single-precision floating-point types.
出处：<a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory" title="Title">cuda-c-programming-guide</a></p></blockquote>

<p>非要加速的话，可以使用一个int2类型的向量拼接成一个double
``` cuda
//declarate the texture using int2 type
texture &lt;int2,1,cudaReadModeElementType> Atex;</p>

<p>//used in global function(cuda kernel)
int2 A1=tex1Dfetch(Atex,(texoff+(ll+offset) <em>lda +ty+offset));
Bs[ty+offset][tx] &ndash;= __hiloint2double(A1.y,A1.x)</em>temp;</p>

<p>```</p>
]]></content>
  </entry>
  
</feed>
